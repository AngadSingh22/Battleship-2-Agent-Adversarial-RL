# PPO Hyperparameters
learning_rate: 2.5e-4
n_steps: 2048       # was 512 — fewer GPU sync points, larger rollout buffers
batch_size: 256     # was 64 — better GPU utilisation per gradient step
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
verbose: 1
